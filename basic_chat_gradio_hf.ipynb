{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneetsaha/popular/blob/master/basic_chat_gradio_hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is a simple example application using Gradio and Hugging Face (HF) hosted LLMs\n",
        "\n",
        "* We will use the OpenAI library to send requests to HF hosted LLMs\n",
        "* Make sure to have HF account and [access token](https://huggingface.co/settings/tokens)"
      ],
      "metadata": {
        "id": "97O0UCZrR12E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the necessary libraries\n",
        "!pip install -q gradio\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "orpMF0gmR0Ws"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By default outputs are limited to a maximum of 1000px of vertical height, but sometimes you expect large outputs but don't want the annoying nested scrollbars.\n",
        "#To remove this limitation, use:\n",
        "from google.colab import output\n",
        "output.no_vertical_scroll()"
      ],
      "metadata": {
        "id": "F4hWBbaGr6mj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b7ec0ded-a80e-4310-81fc-90328b7834ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"7e418342-e9be-11ee-b13b-0242ac1c000c\"] = google.colab.output.setIframeHeight(-1, true, {\"interactive\": true, \"maxHeight\": 99999});\n",
              "//# sourceURL=js_d616a11d49"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the necessary import statements\n",
        "import json\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "33_l_kocTYRw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "HF_ACCESS_TOKEN = userdata.get('HTOKEN')"
      ],
      "metadata": {
        "id": "F8RvYhtBTK4E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the constants for the HF LLM API URLs\n",
        "HF_MIXTRAL_API_URL = \"https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO/v1/\"\n",
        "HF_ZEPHYR_API_URL = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta/v1/\""
      ],
      "metadata": {
        "id": "-fjqkJGkSzgF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_mixtral_client = OpenAI(base_url=HF_MIXTRAL_API_URL, api_key=HF_ACCESS_TOKEN)\n",
        "hf_zephyr_client = OpenAI(base_url=HF_ZEPHYR_API_URL, api_key=HF_ACCESS_TOKEN)"
      ],
      "metadata": {
        "id": "gdjmMcEzTwtF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hf_llm_chat(client, new_messages,  temperature, max_tokens):\n",
        "    print(client.base_url)\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"tgi\",\n",
        "        messages=new_messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return completion\n",
        "\n",
        "def build_messages(prompt, system_prompt):\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    if system_prompt:\n",
        "      messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    for msg in messages:\n",
        "      json_str = json.dumps(msg)\n",
        "      print(\"msg: \", json_str)\n",
        "\n",
        "    return messages\n",
        "\n",
        "def chat_with_mixtral(prompt, temperature=0.7, max_tokens=500, system_prompt=\"\"):\n",
        "  messages = build_messages(prompt, system_prompt)\n",
        "\n",
        "  response = hf_llm_chat(hf_mixtral_client, messages, temperature, max_tokens)\n",
        "  return response.choices[0].message.content\n",
        "\n",
        "def chat_with_zephyr(prompt,  temperature=0.7, max_tokens=500, system_prompt=\"\"):\n",
        "  messages = build_messages(prompt, system_prompt)\n",
        "  response = hf_llm_chat(hf_zephyr_client, messages, temperature, max_tokens)\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "um1XMLoWT5mg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"why is the earth round?\"\n",
        "\n",
        "#response = chat_with_zephyr(prompt)\n",
        "response = chat_with_zephyr(prompt, system_prompt=\"You are a funny astronomer\")\n",
        "\n",
        "to_markdown(response)"
      ],
      "metadata": {
        "id": "ytthqT4PVx4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "424299db-df5c-4b21-906e-1ae68c25230e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "msg:  {\"role\": \"user\", \"content\": \"why is the earth round?\"}\n",
            "msg:  {\"role\": \"system\", \"content\": \"You are a funny astronomer\"}\n",
            "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta/v1/\n",
            "CPU times: user 48.1 ms, sys: 4.06 ms, total: 52.1 ms\n",
            "Wall time: 4.18 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> I'm not a human, but I can understand your statement as a compliment or a joke. If it's meant as a compliment, thank you! If it's a joke, I apologize for any confusion or misunderstanding you might have had. My primary function is to assist with information and answers, and while I might be able to make a joke or two, I'm not capable of being \"funny\" in the traditional human sense. I'm here to provide helpful answers to your questions!"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "prompt = \"why is the earth round?\"\n",
        "#response = chat_with_mixtral(prompt)\n",
        "response = chat_with_mixtral(prompt, system_prompt=\"You are a funny astronomer\")\n",
        "\n",
        "to_markdown(response)"
      ],
      "metadata": {
        "id": "rnkDVIFnVGgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "9bb42628-8da6-4365-b78c-83cc7361895a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "msg:  {\"role\": \"user\", \"content\": \"why is the earth round?\"}\n",
            "msg:  {\"role\": \"system\", \"content\": \"You are a funny astronomer\"}\n",
            "https://api-inference.huggingface.co/models/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO/v1/\n",
            "CPU times: user 44.1 ms, sys: 2.1 ms, total: 46.2 ms\n",
            "Wall time: 4.05 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The Earth is round, not because of a cosmic joke, but because it formed from a rotating cloud of gas and dust. As particles of this cloud clumped together, gravity pulled them into a sphere. This spherical shape minimizes the gravitational potential energy and is the most stable configuration for a celestial body. So, it's not a joke, but a result of gravity and natural forces."
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we are going to add the Gradio user interface to chat w/ HF LLMs\n",
        "\n",
        "* ChatInferface [documentation](https://www.gradio.app/docs/chatinterface]"
      ],
      "metadata": {
        "id": "zenCtqePZAib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This first one is using ChatInterface with minimum customization and it uses the Mixtral LLM"
      ],
      "metadata": {
        "id": "wLWPX308duoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a specific handler for mixstral_chatbot\n",
        "def mixstral_chatbot_hanlder(message, history):\n",
        "  return chat_with_mixtral(message)"
      ],
      "metadata": {
        "id": "LQhMkwTaZyLY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat with Mixtral\n",
        "mixstral_chatbot = gr.ChatInterface(mixstral_chatbot_hanlder, title=\"Welcome Mixstral Chatbot\", undo_btn=None)\n",
        "\n",
        "gr.close_all()\n",
        "\n",
        "mixstral_chatbot.launch(debug=False)"
      ],
      "metadata": {
        "id": "911L48-yZIBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "ec29dfd7-24a7-474f-85b2-732ac773d547"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c6a6d6f137b201e8fa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c6a6d6f137b201e8fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This second one is using ChatInterface with somes customizations and it uses the Zephyr LLM"
      ],
      "metadata": {
        "id": "O9e6WxAzd9Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a specific handler for mixstral_chatbot\n",
        "def zephyr_chatbot_hanlder(message, history):\n",
        "  return chat_with_zephyr(message)"
      ],
      "metadata": {
        "id": "jlHAtZfjcTPk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat with Zephyr\n",
        "zephyr_chatbot = gr.ChatInterface(zephyr_chatbot_hanlder,\n",
        "                                  textbox=gr.Textbox(placeholder=\"Ask anything\"),\n",
        "                                  title=\"Welcome Zephyr Chatbot\",\n",
        "                                  undo_btn=None, retry_btn=None, clear_btn=None)\n",
        "\n",
        "gr.close_all()\n",
        "\n",
        "zephyr_chatbot.launch(debug=True)"
      ],
      "metadata": {
        "id": "eTfxE3egbmsr",
        "outputId": "1dc31167-9330-4110-8a8a-cba5e199d48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Expected a gr.Textbox or gr.MultimodalTextbox component, but got <class 'gradio.components.textbox.Textbox'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-5f3ff094a839>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# chat with Zephyr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m zephyr_chatbot = gr.ChatInterface(zephyr_chatbot_hanlder,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                   \u001b[0mtextbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Ask anything\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Welcome Zephyr Chatbot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   undo_btn=None, retry_btn=None, clear_btn=None)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fn, multimodal, chatbot, textbox, additional_inputs, additional_inputs_accordion_name, additional_inputs_accordion, examples, cache_examples, title, description, theme, css, js, head, analytics_enabled, submit_btn, stop_btn, retry_btn, undo_btn, clear_btn, autofocus, concurrency_limit, fill_height, delete_cache)\u001b[0m\n\u001b[1;32m    210\u001b[0m                             \u001b[0mtextbox_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultimodalTextbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                         ):\n\u001b[0;32m--> 212\u001b[0;31m                             raise TypeError(\n\u001b[0m\u001b[1;32m    213\u001b[0m                                 \u001b[0;34mf\"Expected a gr.Textbox or gr.MultimodalTextbox component, but got {type(textbox_)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a gr.Textbox or gr.MultimodalTextbox component, but got <class 'gradio.components.textbox.Textbox'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For the final example chatbot, we will put the chatbot UI together in a custom way and it will include addition capabilities, such as\n",
        "* System prompt\n",
        "* Side by side comparison"
      ],
      "metadata": {
        "id": "Msf2HiRWlk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_duet_handler(prompt, system_prompt):\n",
        "  mixtral_response = chat_with_mixtral(prompt, system_prompt=system_prompt)\n",
        "  zephyr_response = chat_with_zephyr(prompt, system_prompt=system_prompt)\n",
        "\n",
        "  return mixtral_response, zephyr_response, \"\""
      ],
      "metadata": {
        "id": "bljQQj-qqBGI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See this link more various themes to try out - https://huggingface.co/spaces/gradio/theme-gallery\n",
        "with gr.Blocks(theme=\"bethecloud/storj_theme\", css=\".gradio-container {background-color: #938bd6}\") as chatbot_duet_app:\n",
        "    gr.Label(\"Welcome to GalaxiaBuddy\", color=\"#d1d9f0\")\n",
        "\n",
        "    system_prompt_tb = gr.Textbox(label=\"System Prompt\", lines=2, value=\"You are an helpful and funny assistant with expertise in astronomy and philosophy\")\n",
        "\n",
        "    with gr.Row():\n",
        "      mixtral_response_tb = gr.Textbox(label=\"Mixtral Response\", lines=8)\n",
        "     # zephyr_response_tb = gr.Textbox(label=\"Zephyr Response\", lines=8)\n",
        "\n",
        "    with gr.Row():\n",
        "      prompt_tb = gr.Textbox(label=\"Prompt\", placeholder=\"Ask away\")\n",
        "    with gr.Row():\n",
        "      submit_btn = gr.Button(value=\"Submit\")\n",
        "\n",
        "    prompt_tb.submit(fn=chatbot_duet_handler, inputs=[prompt_tb, system_prompt_tb], outputs=[mixtral_response_tb, zephyr_response_tb, prompt_tb])\n",
        "    submit_btn.click(fn=chatbot_duet_handler, inputs=[prompt_tb, system_prompt_tb], outputs=[mixtral_response_tb, zephyr_response_tb, prompt_tb])\n",
        "\n",
        "gr.close_all()\n",
        "chatbot_duet_app.launch(debug=True, height=800)\n"
      ],
      "metadata": {
        "id": "TNrRb7yJloyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8e8d60d-2c71-4213-93db-f1398a2a8170"
      },
      "execution_count": 31,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c3951ba49b2f7242ed.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c3951ba49b2f7242ed.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c6a6d6f137b201e8fa.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://c3951ba49b2f7242ed.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}